{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f7f73b-1f44-43c8-b607-406638e7e33a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Session 5: Building a model from scratch (II)\n",
    "\n",
    "Previously...\n",
    "\n",
    "<p style=\"font-size:120%;font-style:italic\">‚Ä¢ Given an ECG signal, develop a method to classify each beat by its origin in the cardiac tissue</p>\n",
    "\n",
    "Then we explored, understood, cleaned and homogeneized the data.\n",
    "\n",
    "And when we tried a first simple machine learning model:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"padding-right: 30pt;\">\n",
    "            <table>\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th></th>\n",
    "                        <th>Pred 0</th>\n",
    "                        <th>Pred 1</th>\n",
    "                        <th>Pred 2</th>\n",
    "                        <th>Pred 3</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    <tr>\n",
    "                        <td>True 0</td>\n",
    "                        <td>2920</td>\n",
    "                        <td>0</td>\n",
    "                        <td>6</td>\n",
    "                        <td>0</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>True 1</td>\n",
    "                        <td>51</td>\n",
    "                        <td>57</td>\n",
    "                        <td>0</td>\n",
    "                        <td>0</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>True 2</td>\n",
    "                        <td>44</td>\n",
    "                        <td>0</td>\n",
    "                        <td>182</td>\n",
    "                        <td>3</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>True 3</td>\n",
    "                        <td>6</td>\n",
    "                        <td>0</td>\n",
    "                        <td>3</td>\n",
    "                        <td>10</td>\n",
    "                    </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </td>\n",
    "        <td>\n",
    "            <table>\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th></th>\n",
    "                        <th>Precision</th>\n",
    "                        <th>Recall</th>\n",
    "                        <th>F1-Score</th>\n",
    "                        <th>Support</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    <tr>\n",
    "                        <td>0</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>1.00</td>\n",
    "                        <td>0.98</td>\n",
    "                        <td>2926.00</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>1</td>\n",
    "                        <td>1.00</td>\n",
    "                        <td>0.53</td>\n",
    "                        <td>0.69</td>\n",
    "                        <td>108.00</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>2</td>\n",
    "                        <td>0.95</td>\n",
    "                        <td>0.79</td>\n",
    "                        <td>0.87</td>\n",
    "                        <td>229.00</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>3</td>\n",
    "                        <td>0.77</td>\n",
    "                        <td>0.53</td>\n",
    "                        <td>0.62</td>\n",
    "                        <td>19.00</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Accuracy</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>0.97</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Macro Avg</td>\n",
    "                        <td>0.92</td>\n",
    "                        <td>0.71</td>\n",
    "                        <td>0.79</td>\n",
    "                        <td>3282.00</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>Weighted Avg</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>0.97</td>\n",
    "                        <td>0.96</td>\n",
    "                        <td>3282.00</td>\n",
    "                    </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<p style=\"font-size:400%;color:darkred;text-align:center\">ü•≥</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b3c5e-b48b-47dc-88ad-4db6c623745b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "---- \n",
    "<p style=\"font-size:400%;color:darkred;text-align:center\">‚ö†Ô∏è‚ö†Ô∏è WRONG!!! ‚ö†Ô∏è‚ö†Ô∏è</p>\n",
    "\n",
    "The last, quick experiment with a simple ML model is **fundamentally wrong**, and any conclusion we may get from it will be misleading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba389ffb-6106-4a50-b9de-9ce018078ec3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### **What happened?**\n",
    "\n",
    " - After we segmented and homogeneized the raw signal for each individual heartbeat, we created a big `X` matrix with one row per beat, and proceeded in the standard way:\n",
    "\n",
    "<pre>\n",
    "<code class=\"language-python\">\n",
    "# Prepare the X and y matrices\n",
    "X = np.hstack(df_sample2.beat_sig).T\n",
    "X = X - np.median(X, axis=1, keepdims=True)\n",
    "y = np.repeat(df_sample2.type, 2)\n",
    "# Split in training and test\n",
    "<span style=\"background-color: #ffff99;\">X_train, X_test, y_train, y_test = <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a>(X, y, test_size=0.2, random_state=42)</span>\n",
    "# Create the model, train it and test it\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "However, the core of any Machine Learning methodology is the assumption that **`X_train` and `X_test` are independent and indentically distributed**. We have violated the independence assumption in two different ways:\n",
    " - By including beats **from the same subject** in training and testing.\n",
    " - By including **the same beat** (with different projections corresponding to the leads) in training and testing.\n",
    "\n",
    "This problem, commonly know as **data leakage**, is pervasive in real studies using ML, and the main cause of overestimated results, even in peer-reviewed publications.\n",
    " - Example: [Ankƒ±≈ühan et.al: *Early stage lung cancer detection from speech sounds in natural environments* (2024)](https://tomas-teijeiro.github.io/Slides_BCAM/#/5/5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ed1b3-ebcb-46f3-917f-18838f8ee3e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's recall the procedure for addressing any realistic problem with Machine Learning:\n",
    " 1. Learning about the problem to be solved.\n",
    " 2. Understanding the available data.\n",
    " 3. Data cleaning, preparation and homogeneization.\n",
    " 4. **Design of experimental validation.**\n",
    " 5. Choose a model or set of models to evaluate.\n",
    " 6. Training and evaluating the model(s).\n",
    " 7. Extract conclusions, assess the limitations of the model.\n",
    "\n",
    "The problem with step 4 is that it is totally dependent on the **specific problem** to be solved, and using the same strategy for two problems that in principle look similar may be catastrophic. \n",
    " - For example, mixing data from the same patient in training and testing can be perfectly fine, and even required, if we are building a personalized model.\n",
    "\n",
    "The following code cells load and prepare the data as we did it in the last session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee1b27c-383d-47c9-b02a-5e9a40fa789d",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "\n",
    "#General imports\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wfdb\n",
    "import keras\n",
    "#Figure settings to avoid super large plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 90\n",
    "#Local path to the MIT-BIH Arrhythmia\n",
    "MITDB = '/opt/tljh/common/mitdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10d15b9-dffc-402b-a453-c65e5a809647",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Global structures with target labels, mapping to classes, and list of recordings to analyze\n",
    "BEAT_CODES = set(['N','L','R','B','A','a','J','S','V','F','e','j','n','E','/','f','Q'])\n",
    "LABEL_MAP = {'N':0,'L':0,'R':0,'B':0,\n",
    "             'A':1,'a':1,'J':1,'S':1,'e':1,'j':1,'n':1,\n",
    "             'V':2,'E':2,\n",
    "             'F':3,\n",
    "             '/':4,'f':4,'Q':4}\n",
    "REC_LIST = [r.strip() for r in open(f'{MITDB}/RECORDS', 'r').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c9103-f030-45cb-ade4-f8f512619899",
   "metadata": {},
   "source": [
    "In Session 4, we agreed that we should remove leads `V5`, `V2` and `V4` due to the extremely small number of samples compared with the other leads. However, if we are doing the split at patient level, we need to consider all existing **lead combinations**. Let's get a quick summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a5595b-69d0-43f4-a896-7fb8dbb2072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('MLII', 'V1'): 40,\n",
       "         ('MLII', 'V5'): 2,\n",
       "         ('V5', 'V2'): 2,\n",
       "         ('MLII', 'V2'): 2,\n",
       "         ('V5', 'MLII'): 1,\n",
       "         ('MLII', 'V4'): 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter([tuple(wfdb.rdrecord(f'{MITDB}/{r}').sig_name) for r in REC_LIST])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6d8b1-b32a-4661-b9ba-2411de94e7ab",
   "metadata": {},
   "source": [
    "It seems evident we should select the recordings with leads `(MLII, V1)`. Let's filter them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c20aec-75eb-4edb-99d4-a280ef5bb112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101', '105', '106', '107', '108', '109', '111', '112', '113', '115', '116', '118', '119', '121', '122', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234'] 40\n"
     ]
    }
   ],
   "source": [
    "REC_LIST = [r for r in REC_LIST if tuple(wfdb.rdrecord(f'{MITDB}/{r}').sig_name)==('MLII', 'V1')]\n",
    "print(REC_LIST, len(REC_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a334ac70-899f-40d1-ae40-61f758f7d978",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Code from the last session to load and organize every beat in a global dataframe\n",
    "def adjust_array_length(arr, peak):\n",
    "    \"\"\"Utility function to adjust the length of each array\"\"\"\n",
    "    desired_length = 1501\n",
    "    target_peak_index = 750\n",
    "    current_length = len(arr)\n",
    "\n",
    "    start_index = max(0, peak - target_peak_index)\n",
    "    end_index = min(current_length, peak + (desired_length - target_peak_index))\n",
    "\n",
    "    # Truncate the array around the peak\n",
    "    truncated_array = arr[start_index:end_index, :]\n",
    "\n",
    "    left_padding = target_peak_index - (peak - start_index)\n",
    "    right_padding = desired_length - len(truncated_array) - left_padding\n",
    "\n",
    "    # Extend the array if needed\n",
    "    if left_padding > 0:\n",
    "        left_pad_value = truncated_array[0, :]\n",
    "        left_extension = np.full((left_padding, 2), left_pad_value)\n",
    "    else:\n",
    "        left_extension = np.empty(shape=(0, 2))\n",
    "\n",
    "    if right_padding > 0:\n",
    "        right_pad_value = truncated_array[-1, :]\n",
    "        right_extension = np.full((right_padding, 2), right_pad_value)\n",
    "    else:\n",
    "        right_extension = np.empty(shape=(0, 2))\n",
    "\n",
    "    # Combine the extensions and truncated array\n",
    "    adjusted_array = np.concatenate((left_extension, truncated_array, right_extension))\n",
    "\n",
    "    return adjusted_array\n",
    "    \n",
    "# Creation of a Pandas dataframe with homogeneized data for every beat\n",
    "full_data = []\n",
    "offset = 18 #Window cut with respect to previous and next peak.\n",
    "for r in REC_LIST:\n",
    "    rec = wfdb.rdrecord(f'{MITDB}/{r}')\n",
    "    anns = wfdb.rdann(f'{MITDB}/{r}', 'atr')\n",
    "    beat_mask = np.array([s in BEAT_CODES for s in anns.symbol])\n",
    "    beat_indices = np.where(beat_mask)[0]\n",
    "    beat_types = np.array(anns.symbol)[beat_indices]\n",
    "    beat_locations = anns.sample[beat_indices]\n",
    "    for i in range(1, len(beat_indices)-1):\n",
    "        beat_data = {}\n",
    "        beat_data['beat_sig'] = adjust_array_length(rec.p_signal[beat_locations[i-1]\n",
    "                                                    + offset:beat_locations[i+1]-offset,:],\n",
    "                                                    beat_locations[i]-beat_locations[i-1])\n",
    "        beat_data['type'] = LABEL_MAP[beat_types[i]]\n",
    "        beat_data['leads'] =  rec.sig_name\n",
    "        beat_data['recname'] = rec.record_name\n",
    "        full_data.append(beat_data)\n",
    "df_beats = pd.DataFrame(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953d696-d1f9-4029-ab5f-9d24db1417dd",
   "metadata": {},
   "source": [
    "We also concluded that class 4 should be removed from the analysis, basically for the very same reason that we removed some of the leads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee0f535-a78c-410c-9d9a-6a3626ff2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beats = df_beats.query('type!=4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23405e56-d479-4179-a01a-0d080afea672",
   "metadata": {},
   "source": [
    "As a general recommendation, while you are developing and exploring initial models, do it with a restricted amount of data to speed-up things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1482c4b7-c6c5-42af-ab4b-747dcb1aec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_beats.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e734caf-758a-41cf-9fb6-4c585d96b115",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### üìã Exercise 1\n",
    "\n",
    "Define, build, train and validate a Machine Learning model to solve the heartbeat classification problem, considering all the issues we overlooked in Session 4.\n",
    "\n",
    " - The main metric to assess the quality of the model will be **weighted average F1 score** for the 4 classes.\n",
    "\n",
    "__Hints:__\n",
    " - A CNN is probably the most promising model a priori. You may use a 2D CNN in case you want to input the two leads of each beat simultaneously, or a 1D CNN in case you want to use single leads.\n",
    "     - Suggested architecture to get started:\n",
    "         - 3 1D-Convolutional layers, with filter size 64, 128 and 256 (kernel size 3), each one followed by BatchNormalization and MaxPooling1D (with pool size 2).\n",
    "         - 2 Dense layers (after a Flatten one), with 128 and 64 units. Dropout of 0.5 after each of them.\n",
    "         - Output Dense layer, with 4 outputs (one per class).\n",
    " - Be extremely careful with the data splitting strategy. Make sure statistical independence is maintained between training, validation and test.\n",
    " - Start from a simple model, get a working pipeline, and optimize over that. Even the one tested in Session 4 could do the job for this.\n",
    " - Class imbalance may be a big problem when training. Consider playing with the `class_weight` and `sample_weight` parameters of `model.fit()`.\n",
    "     - Accuracy is usually not a good metric to follow in imbalanced scenarios.\n",
    " - Once you have a model you are happy with, train it also to work with a **single lead** and compare the results.\n",
    " - Rhythm information (distance to the next and previous beats) is essential for a proper detection of some classes. While this information is already in the data format we developed, it may be difficult to extract automatically. Explicitly providing these values as features can help.\n",
    " - The ideal validation pipeline for this kind of problems with limited amount of subjects is leave-one-subject out cross-validation, or at least k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061f6ba-0e81-4fe8-bfc8-c7b6a59eb08b",
   "metadata": {},
   "source": [
    "In order to guarantee the independence of training and test sets, we will make the split ensuring no data from the same recording are in training and testing. The [`sklearn.model_selection.GroupShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html) splitter is specifically designed for these use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f91511c5-c7e7-4255-bece-1bb98a0ad7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#We get 80% of the beats for training and 20% for testing, ensuring recordings are different in\n",
    "#both sets.\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gsplit = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_index, test_index = next(gsplit.split(df_sample.beat_sig, df_sample.type, df_sample.recname))\n",
    "#Check that the split was done indeed in a proper way\n",
    "print(set(df_sample.iloc[train_index].recname.unique()).isdisjoint(set(df_sample.iloc[test_index].recname.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb8e4e87-40f8-488c-a47e-2b8241b38429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data to the classical X, y training and testing matrices.\n",
    "X_train = np.stack(df_sample.iloc[train_index].beat_sig.values)\n",
    "y_train = df_sample.iloc[train_index].type.values\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "X_test = np.stack(df_sample.iloc[test_index].beat_sig.values)\n",
    "y_test = df_sample.iloc[test_index].type.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d225ec69-7c3b-436f-b21f-452d038b3888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7739, 1501, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd60b6-55bd-42d9-a844-444efaf0e3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
